{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocean Biogeochemical Dynamics Lab, Spring 2021\n",
    "Doing simple seasonal decomposition of drivers of DIC and TA with SOCCOM floats by Nancy Williams\n",
    "\n",
    "This assumes you have already become familiar with `OBD_SOCCOM_L3.ipynb` and `OBD_SOCCOM_CO2SYS.ipynb` notebooks.\n",
    "\n",
    "SOCCOM website: https://soccom.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "import seaborn as sns # this will change the look of pandas plots, too\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import seawater\n",
    "import xarray as xr\n",
    "from PyCO2SYS.api import CO2SYS_wrap as co2sys\n",
    "# this forces matplotlib to print figures out here when you make plots\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.rcParams['font.size'] = 18\n",
    "#plt.rcParams['lines.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want the figures to be saved\n",
    "output_dir = 'generated/'\n",
    "# you could use os.path.join() which adds slashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can switch this to turn debugging on or off (True or False)\n",
    "DEBUG = False\n",
    "\n",
    "def debug (*args):\n",
    "    \"\"\"Call print() with arguments if DEBUG is True\"\"\" # preserves formatting\n",
    "    if DEBUG:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset\n",
    "We want to use Pandas' built-in read_csv function to import a single float data file into a pandas data frame called `flt`. Float 9254 is a good example float for this exercise, but you can pick any float from the snapshot. To pick another float you can go to the SOCCOM web page sensor status table http://soccom.ucsd.edu/floats/SOCCOM_sensor_stats.html and choose a float with lots of good data (i.e., more in the \"#g\" column than the \"#b\" columns for variables you're interested in analyzing. You can sort the columns on that webpage by clicking on the column header by which you wish to sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a float\n",
    "floatnum = '9254' \n",
    "floatpath = 'SOCCOM_LoResQC_LIAR_22Dec2020_odvtxt/' # This is the folder where all the float data live\n",
    "floatsuffix = 'SOOCNQC.TXT' # all of the Southern Ocean floats have the same suffix\n",
    "\n",
    "# Change // comment symbols to #\n",
    "# input file\n",
    "fin = open(floatpath+floatnum+floatsuffix,'rt',encoding='UTF-8')\n",
    "\n",
    "# output file to which we will write the result\n",
    "fout = open('fltrem.txt','wt')\n",
    "\n",
    "# this is a for loop\n",
    "# for each line in the input file\n",
    "for line in fin:\n",
    "    # red and replace the string and write to the output file\n",
    "    fout.write(line.replace('//','#'))\n",
    "# close the files\n",
    "fin.close()\n",
    "fout.close()\n",
    "\n",
    "flt = pd.read_csv(\n",
    "    'fltrem.txt',\n",
    "    delimiter='\\t',\n",
    "    comment='#',\n",
    "    na_values=-1E10,\n",
    ")\n",
    "\n",
    "flt['date'] = pd.to_datetime(flt['mon/day/yr'] + ' ' + flt['hh:mm'])\n",
    "debug(flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all Quality Flag columns and apply them to the preceeding columns\n",
    "from re import search\n",
    "for column in range(len(flt.columns)):\n",
    "    name=flt.columns[column]\n",
    "    if search('QF',name): # if the column is a QF column, apply it to the preceeding column, otherwise go on to next column\n",
    "        var=flt.columns[column-1]\n",
    "        flt[var] = np.where(flt.iloc[:,flt.columns.get_loc(var)+1] == 0,flt[var], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping your data\n",
    "It's always a good idea to map your data and make sure it is where you think it is. Here we will use cartopy (basemap is deprecated).\n",
    "\n",
    "Since we're talking about the Southern Ocean and there are stark fronts, it's good to plot your data in relation to these fronts. The climatological locations of the fronts are available from Orsi et al. (1995) https://www.sciencedirect.com/science/article/pii/096706379500021W. Text files containing the locations of the fronts are located in the \"fronts\" folder.\n",
    "\n",
    ">Orsi, A. H., Whitworth, T. I., & Nowlin, W. D. J. (1995). On the meridional extent and fronts of the Antarctic Circumpolar Current. Deep Sea Research Part I. https://doi.org/10.1016/0967-0637(95)00021-W\n",
    "\n",
    "### Where is your float in relation to the fronts of the ACC?\n",
    "The following code imports the longitudes and latitudes of the five fronts. There are some '%' values in the files which creates breaks in the fronts. If we did not keep these breaks, the fronts would plot across continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_front_csv(name):\n",
    "    \"\"\"read Orsi front csv file into pandas dataframe\"\"\"\n",
    "    return pd.read_csv(\n",
    "        f'fronts/{name}.txt', #fstrings\n",
    "        header=None,\n",
    "        sep='\\s+',\n",
    "        na_values='%',\n",
    "        names=['lon', 'lat'],\n",
    "    )\n",
    "\n",
    "stf = read_front_csv('stf')\n",
    "saf = read_front_csv('saf')\n",
    "pf = read_front_csv('pf')\n",
    "saccf = read_front_csv('saccf')\n",
    "sbdy = read_front_csv('sbdy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of a South Polar Stereographic map using Cartopy https://scitools.org.uk/cartopy/docs/latest/#. Polar stereographic maps are always a bit more complicated because you have to translate your coordinates to polar coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(6, 6))\n",
    "ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "ax.set_extent([-180,180,-90,-30], ccrs.PlateCarree())\n",
    "ax.add_feature(cartopy.feature.LAND)\n",
    "ax.add_feature(cartopy.feature.OCEAN)\n",
    "ax.gridlines()\n",
    "\n",
    "# Compute a circle in axes coordinates, which we can use as a boundary\n",
    "# for the map. We can pan/zoom as much as we like - the boundary will be\n",
    "# permanently circular.\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "center, radius = [0.5, 0.5], 0.5\n",
    "verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n",
    "circle = mpath.Path(verts * radius + center)\n",
    "\n",
    "ax.set_boundary(circle, transform = ax.transAxes)\n",
    "plt.plot(stf['lon'], stf['lat'], color='Red', transform=ccrs.PlateCarree())\n",
    "plt.plot(saf['lon'], saf['lat'], color='Orange', transform=ccrs.PlateCarree())\n",
    "plt.plot(pf['lon'], pf['lat'], color='Yellow', transform=ccrs.PlateCarree())\n",
    "plt.plot(saccf['lon'], saccf['lat'], color='Green', transform=ccrs.PlateCarree())\n",
    "plt.plot(sbdy['lon'], sbdy['lat'], color='Blue', transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.scatter(flt['Lon [°E]'],flt['Lat [°N]'], color='Black', transform=ccrs.PlateCarree(), s=1)\n",
    "plt.savefig(output_dir + 'F' +floatnum + 'map.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MLD for each station\n",
    "MLD = []\n",
    "for station in flt['Station'].unique():\n",
    "    surfacedens = flt['Sigma_theta[kg/m^3]'].loc[(flt['Station']==station)].min()\n",
    "    MLD.append(\n",
    "        [station,flt['date'].loc[(flt['Station']==station)&\n",
    "                                 (flt['Sigma_theta[kg/m^3]']-surfacedens>0.03)].min(),\n",
    "         flt['Depth[m]'].loc[(flt['Station']==station)&\n",
    "                             (flt['Sigma_theta[kg/m^3]']-surfacedens>0.03)].min(),\n",
    "         flt['Lon [°E]'].loc[(flt['Station']==station)].mean(),\n",
    "         flt['Lat [°N]'].loc[(flt['Station']==station)].mean()])\n",
    "\n",
    "# Take a look at MLD. First column is the Station, second column is datenum, second column is the MLD\n",
    "MLD = pd.DataFrame(data=MLD, columns=['Station', 'date', 'MLD','Lon [°E]','Lat [°N]'])\n",
    "debug(MLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a time series plot of average mixed layer observations\n",
    "Here we first pull *only* the data from the mixed layer (shallower than MLD) and put into a separate dataframe called `fltSurf`. We then use the pandas groupby function to group and average the mixed layer data by station.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltSurf = pd.DataFrame()\n",
    "for station in flt['Station'].unique():\n",
    "    stationMLD = MLD.MLD[MLD.Station==station].values[0]\n",
    "    mask = (flt['Station']==station) & (flt['Pressure[dbar]']<stationMLD)\n",
    "    fltSurf = fltSurf.append(flt[mask])\n",
    "debug(fltSurf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby to get average values for each station\n",
    "fltSurfByStn = fltSurf.groupby('Station', as_index=False).mean()\n",
    "debug(fltSurfByStn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby doesn't work on datetime column so it was dropped. We need to make a datetime array to be used with plotting\n",
    "fltdates = []\n",
    "for station in fltSurfByStn['Station'].unique():\n",
    "    fltdates.append([flt['date'].loc[(flt['Station']==station)].min()])\n",
    "debug(len(fltdates))\n",
    "debug(type(fltdates))\n",
    "fltdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a big plot with subplots\n",
    "fig,axes = plt.subplots(nrows = 7, ncols = 1,figsize=(15,25))\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "var='Temperature[°C]'\n",
    "axes[0].plot(fltdates,fltSurfByStn[var])\n",
    "axes[0].set_ylabel(var)\n",
    "axes[0].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[0].xaxis.set_major_locator(years)\n",
    "axes[0].xaxis.set_major_formatter(years_fmt)\n",
    "axes[0].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='Salinity[pss]'\n",
    "axes[1].plot(fltdates,fltSurfByStn[var])\n",
    "axes[1].set_ylabel(var)\n",
    "axes[1].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[1].xaxis.set_major_locator(years)\n",
    "axes[1].xaxis.set_major_formatter(years_fmt)\n",
    "axes[1].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='Nitrate[µmol/kg]'\n",
    "axes[2].plot(fltdates,fltSurfByStn[var])\n",
    "axes[2].set_ylabel(var)\n",
    "axes[2].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[2].xaxis.set_major_locator(years)\n",
    "axes[2].xaxis.set_major_formatter(years_fmt)\n",
    "axes[2].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='TALK_LIAR[µmol/kg]'\n",
    "axes[3].plot(fltdates,fltSurfByStn[var])\n",
    "axes[3].set_ylabel(var)\n",
    "axes[3].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[3].xaxis.set_major_locator(years)\n",
    "axes[3].xaxis.set_major_formatter(years_fmt)\n",
    "axes[3].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='pHinsitu[Total]'\n",
    "axes[4].plot(fltdates,fltSurfByStn[var])\n",
    "axes[4].set_ylabel(var)\n",
    "axes[4].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[4].xaxis.set_major_locator(years)\n",
    "axes[4].xaxis.set_major_formatter(years_fmt)\n",
    "axes[4].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='pCO2_LIAR[µatm]'\n",
    "axes[5].plot(fltdates,fltSurfByStn[var])\n",
    "axes[5].set_ylabel(var)\n",
    "axes[5].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[5].xaxis.set_major_locator(years)\n",
    "axes[5].xaxis.set_major_formatter(years_fmt)\n",
    "axes[5].xaxis.set_minor_locator(months)\n",
    "\n",
    "var='MLD'\n",
    "axes[6].plot(MLD['date'],MLD['MLD'])\n",
    "axes[6].set_ylabel(var)\n",
    "axes[6].set_xlim(fltdates[0],fltdates[-1])\n",
    "axes[6].xaxis.set_major_locator(years)\n",
    "axes[6].xaxis.set_major_formatter(years_fmt)\n",
    "axes[6].xaxis.set_minor_locator(months)\n",
    "\n",
    "axes[0].set_title('Float ' + floatnum + ' Mixed Layer Properties')\n",
    "fig.savefig(output_dir + 'F' + floatnum + 'MLD.png', dpi = 200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed layer decomposition\n",
    "Next, we will do a simple decomposition of the drivers of DIC and TA over the annual cycle. This simple decomposition does not account for the differences between processes which might add/subtract DIC and TA from the mixed layer through physical processes. \n",
    "\n",
    "The first step is to use the built in pandas.diff() function to take the differences between each station and the last. This will serve as our \"Deltas\" or changes in mixed layer depth properties between stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deltas = fltSurfByStn.diff()\n",
    "debug(Deltas)\n",
    "debug(fltSurfByStn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now what if we want to get really fancy and add the supply of \"things\" from below (entrainment)?\n",
    "# First let's calculate the sub-mixed layer values\n",
    "fltSubMLD = pd.DataFrame()\n",
    "for station in fltSurfByStn['Station'].unique(): \n",
    "    stationMLD = MLD.MLD[MLD.Station==station].values[0]\n",
    "    mask = (flt['Station']==station) & (flt['Pressure[dbar]']>stationMLD) & (flt['Pressure[dbar]']<stationMLD+50)\n",
    "    # this is not very sensitive to which depth below the mixed layer you choose \n",
    "    fltSubMLD = fltSubMLD.append(flt[mask])\n",
    "debug(fltSubMLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltSubMLDByStn = fltSubMLD.groupby('Station',as_index=False).mean()\n",
    "debug(fltSubMLDByStn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When the mixed layer is deepening (dMLD is positive) calculate dN_dMLD\n",
    "dN_dMLD = pd.DataFrame()\n",
    "dTA_dMLD = pd.DataFrame()\n",
    "dDIC_dMLD = pd.DataFrame()\n",
    "for station in fltSurfByStn['Station'].unique():\n",
    "    if station == 1: # to turn off this block, make it read \"if False and ___\"\n",
    "        dN_dMLD = dN_dMLD.append([0], ignore_index=True)\n",
    "        dTA_dMLD = dTA_dMLD.append([0], ignore_index=True)\n",
    "        dDIC_dMLD = dDIC_dMLD.append([0], ignore_index=True)\n",
    "        \n",
    "        lastSubML = fltSubMLDByStn.loc[fltSubMLDByStn['Station'] == station]\n",
    "        lastML = fltSurfByStn.loc[fltSurfByStn['Station'] == station]\n",
    "        lastMLD = MLD.MLD[MLD.Station == station].values[0]\n",
    "        continue\n",
    "    \n",
    "    if (len(lastSubML) == 0) or (len(lastML) == 0):\n",
    "        dN_dMLD = dN_dMLD.append([0], ignore_index=True)\n",
    "        dTA_dMLD = dTA_dMLD.append([0], ignore_index=True)\n",
    "        dDIC_dMLD = dDIC_dMLD.append([0], ignore_index=True)\n",
    "        \n",
    "        lastSubML = fltSubMLDByStn.loc[fltSubMLDByStn['Station'] == station]\n",
    "        lastML = fltSurfByStn.loc[fltSurfByStn['Station'] == station]\n",
    "        lastMLD = MLD.MLD[MLD.Station == station].values[0]\n",
    "        continue\n",
    "    \n",
    "    MLDt2 = MLD.MLD[MLD.Station == station].values[0]\n",
    "    dMLD = MLDt2 - lastMLD\n",
    "    if dMLD > 0: # i.e., only if the mixed layer is deepening\n",
    "        # Calculate the change in mixed layer content from the entrainment\n",
    "        # Use the surface mixed layer and sub-mixed layer values from the last station\n",
    "        dN = ((lastML['Nitrate[µmol/kg]'].values[0] * lastMLD +\n",
    "               lastSubML['Nitrate[µmol/kg]'].values[0] * dMLD) /\n",
    "              MLDt2) - lastML['Nitrate[µmol/kg]'].values[0]\n",
    "        dTA = ((lastML['TALK_LIAR[µmol/kg]'].values[0] * lastMLD +\n",
    "               lastSubML['TALK_LIAR[µmol/kg]'].values[0] * dMLD) /\n",
    "              MLDt2) - lastML['TALK_LIAR[µmol/kg]'].values[0]\n",
    "        dDIC = ((lastML['DIC_LIAR[µmol/kg]'].values[0] * lastMLD +\n",
    "               lastSubML['DIC_LIAR[µmol/kg]'].values[0] * dMLD) /\n",
    "              MLDt2) - lastML['DIC_LIAR[µmol/kg]'].values[0]\n",
    "        \n",
    "        #dN = [(A['Nitrate[µmol/kg]'].values[0] - B['Nitrate[µmol/kg]'].values[0]) * dMLD / MLDt1]\n",
    "        #dTA = [(A['TALK_LIAR[µmol/kg]'].values[0] - B['TALK_LIAR[µmol/kg]'].values[0]) * dMLD / MLDt2]\n",
    "        #dDIC = [(A['DIC_LIAR[µmol/kg]'].values[0] - B['DIC_LIAR[µmol/kg]'].values[0]) * dMLD / MLDt2]\n",
    "        \n",
    "        debug (station, dMLD, dN)\n",
    "        \n",
    "        dN_dMLD = dN_dMLD.append(pd.DataFrame([dN]), ignore_index=True)\n",
    "        dTA_dMLD = dTA_dMLD.append(pd.DataFrame([dTA]), ignore_index=True)\n",
    "        dDIC_dMLD = dDIC_dMLD.append(pd.DataFrame([dDIC]), ignore_index=True)\n",
    "        \n",
    "        lastSubML = fltSubMLDByStn.loc[fltSubMLDByStn['Station'] == station]\n",
    "        lastML = fltSurfByStn.loc[fltSurfByStn['Station'] == station]\n",
    "        lastMLD = MLD.MLD[MLD.Station == station].values[0]\n",
    "      \n",
    "    else:\n",
    "        dN_dMLD = dN_dMLD.append([0], ignore_index=True)\n",
    "        dTA_dMLD = dTA_dMLD.append([0], ignore_index=True)\n",
    "        dDIC_dMLD = dDIC_dMLD.append([0], ignore_index=True)\n",
    "        \n",
    "        lastSubML = fltSubMLDByStn.loc[fltSubMLDByStn['Station'] == station]\n",
    "        lastML = fltSurfByStn.loc[fltSurfByStn['Station'] == station]\n",
    "        lastMLD = MLD.MLD[MLD.Station == station].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug(len(dN_dMLD), len(dTA_dMLD), len(dDIC_dMLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dN_dS = Deltas['Salinity[pss]'] * fltSurfByStn['Nitrate[µmol/kg]'].iloc[0] / fltSurfByStn['Salinity[pss]'].iloc[0]\n",
    "dTA_dS = Deltas['Salinity[pss]'] * fltSurfByStn['TALK_LIAR[µmol/kg]'].iloc[0] / fltSurfByStn['Salinity[pss]'].iloc[0]\n",
    "dDIC_dS = Deltas['Salinity[pss]'] * fltSurfByStn['DIC_LIAR[µmol/kg]'].iloc[0] / fltSurfByStn['Salinity[pss]'].iloc[0]\n",
    "\n",
    "debug(dDIC_dS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Community Metabolism (respiration - photosynthesis)\n",
    "\n",
    "# Squeeze turns this dataframe into a series so we can do math with it and these other series\n",
    "dN_NCM = Deltas['Nitrate[µmol/kg]'] - dN_dS - dN_dMLD.squeeze() \n",
    "C_to_N = 117 / 16\n",
    "TA_to_N = -16 / 16\n",
    "dDIC_NCM = dN_NCM * C_to_N\n",
    "dTA_NCM = dN_NCM * TA_to_N\n",
    "\n",
    "debug(dDIC_NCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcification and Dissolution\n",
    "dTA_CaCO3 = Deltas['TALK_LIAR[µmol/kg]'] - dTA_dS - dTA_NCM - dTA_dMLD.squeeze()\n",
    "DIC_to_TA_calc = 0.5\n",
    "dDIC_CaCO3 = dTA_CaCO3 * DIC_to_TA_calc\n",
    "\n",
    "debug(dTA_CaCO3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual term- this represents physics that we didn't explicitly account for, and gas exchange\n",
    "dDIC_resid = Deltas['DIC_LIAR[µmol/kg]'] - dDIC_dS - dDIC_NCM - dDIC_CaCO3 - dDIC_dMLD.squeeze()\n",
    "dDIC_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a big plot with subplots\n",
    "fig,axes = plt.subplots(nrows = 4, ncols = 1,figsize=(15,15))\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "var='DIC_LIAR[µmol/kg]'\n",
    "axes[0].plot(fltdates,Deltas[var].cumsum(), label='Observed', c='black', linewidth=3)\n",
    "axes[0].set_ylabel('Change in ' + var[:3])\n",
    "axes[0].plot(fltdates,dDIC_dS.cumsum(), label='evap/precip', c='deeppink', linewidth=2)\n",
    "axes[0].plot(fltdates,dDIC_dMLD[0].cumsum(), label='entrainment', c='forestgreen', linewidth=2)\n",
    "axes[0].plot(fltdates,dDIC_NCM.cumsum(), label='net community metabolism', c='cyan', linewidth=2)\n",
    "axes[0].plot(fltdates,dDIC_CaCO3.cumsum(), label='calcification/dissolution', c='mediumorchid', linewidth=2)\n",
    "axes[0].plot(fltdates,dDIC_resid.cumsum(), label='residual (gas exchange + physical)', c='grey', linewidth=2)\n",
    "axes[0].set_xlim(fltdates[0], fltdates[-1])\n",
    "axes[0].xaxis.set_major_locator(years)\n",
    "axes[0].xaxis.set_major_formatter(years_fmt)\n",
    "axes[0].xaxis.set_minor_locator(months)\n",
    "axes[0].legend()\n",
    "\n",
    "var='TALK_LIAR[µmol/kg]'\n",
    "axes[1].plot(fltdates,Deltas[var].cumsum(), label='Observed', c='black', linewidth=3)\n",
    "axes[1].set_ylabel('Change in ' + var[:4])\n",
    "axes[1].plot(fltdates,dTA_dS.cumsum(), label='evap/precip', c='deeppink', linewidth=2)\n",
    "axes[1].plot(fltdates,dTA_dMLD[0].cumsum(), label='entrainment', c='forestgreen', linewidth=2)\n",
    "axes[1].plot(fltdates,dTA_NCM.cumsum(), label='net community metabolism', c='cyan', linewidth=2)\n",
    "axes[1].plot(fltdates,dTA_CaCO3.cumsum(), label='calcification/dissolution', c='mediumorchid', linewidth=2)\n",
    "axes[1].set_xlim(fltdates[0], fltdates[-1])\n",
    "axes[1].xaxis.set_major_locator(years)\n",
    "axes[1].xaxis.set_major_formatter(years_fmt)\n",
    "axes[1].xaxis.set_minor_locator(months)\n",
    "#axes[1].legend()\n",
    "\n",
    "var = 'Nitrate[µmol/kg]'\n",
    "axes[2].plot(fltdates,Deltas[var].cumsum(), label='Observed', c='black', linewidth=3)\n",
    "axes[2].set_ylabel('Change in ' + var[:4])\n",
    "axes[2].plot(fltdates,dN_dS.cumsum(), label='evap/precip', c='deeppink', linewidth=2)\n",
    "axes[2].plot(fltdates,dN_dMLD[0].cumsum(), label='entrainment', c='forestgreen', linewidth=2)\n",
    "axes[2].plot(fltdates,dN_NCM.cumsum(), label='net community metabolism', c='cyan', linewidth=2)\n",
    "axes[2].set_xlim(fltdates[0], fltdates[-1])\n",
    "axes[2].xaxis.set_major_locator(years)\n",
    "axes[2].xaxis.set_major_formatter(years_fmt)\n",
    "axes[2].xaxis.set_minor_locator(months)\n",
    "#axes[2].legend()\n",
    "\n",
    "var = 'MLD'\n",
    "axes[3].plot(MLD['date'],MLD['MLD'], label='Observed', c='black', linewidth=3)\n",
    "axes[3].set_ylabel(var)\n",
    "axes[3].set_xlim(fltdates[0], fltdates[-1])\n",
    "axes[3].xaxis.set_major_locator(years)\n",
    "axes[3].xaxis.set_major_formatter(years_fmt)\n",
    "axes[3].xaxis.set_minor_locator(months)\n",
    "\n",
    "axes[0].set_title('Float ' + floatnum + ' Mixed Layer Decomposition')\n",
    "fig.savefig(output_dir + 'F' + floatnum + 'MLDecomp.png', dpi = 200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
